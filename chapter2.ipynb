{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: The text processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "The first action an NLP application performs on a text is parsing text into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'flying', 'to', 'Frisco']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I am flying to Frisco')\n",
    "print([w.text for w in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "The process of reducing word forms to their lemma. The lemma is the base form of the word which would be found in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this this\n",
      "product product\n",
      "integrates integrate\n",
      "both both\n",
      "libraries library\n",
      "for for\n",
      "downloading download\n",
      "                     \n",
      "and and\n",
      "applying apply\n",
      "patches patch\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('this product integrates both libraries for downloading \\\n",
    "          and applying patches')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using lemmatization for meaning recognition\n",
    "If we reduce words to their lemmas, it makes it easier extract necessary information from input. We can also add special cases to the tokenizer for things like nicknames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'flying', 'to', 'Frisco']\n",
      "['I', 'be', 'fly', 'to', 'San Francisco']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH, LEMMA\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I am flying to Frisco') \n",
    "print([w.text for w in doc])\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"TEXT\": \"Frisco\"}]], {\"LEMMA\": \"San Francisco\"})\n",
    "print([w.lemma_ for w in nlp(u'I am flying to Frisco')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using parts of speech to find relevant verbs\n",
    "\n",
    "We can filter to find the infinitive and present progressive forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flying']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I have flown to LA. Now I am flying to Frisco')\n",
    "print([w.text for w in doc if w.tag_ == 'VBG' or w.tag_ == 'VB'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the proper nouns in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LA', 'Frisco']\n"
     ]
    }
   ],
   "source": [
    "print([w.text for w in doc if w.pos_ == 'PROPN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the synatactic dependency labels of the tokens in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON nsubj\n",
      "have AUX aux\n",
      "flown VERB ROOT\n",
      "to ADP prep\n",
      "LA PROPN pobj\n",
      ". PUNCT punct\n",
      "Now ADV advmod\n",
      "I PRON nsubj\n",
      "am AUX aux\n",
      "flying VERB ROOT\n",
      "to ADP prep\n",
      "Frisco PROPN pobj\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I have flown to LA. Now I am flying to Frisco')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What it doesnt show is how words are related to each other in a sentence. This is called a dependency arc. To look at the dependency arcs in the discourse, we can use the loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flown nsubj I\n",
      "flown aux have\n",
      "flown ROOT flown\n",
      "flown prep to\n",
      "to pobj LA\n",
      "flown punct .\n",
      "flying advmod Now\n",
      "flying nsubj I\n",
      "flying aux am\n",
      "flying ROOT flying\n",
      "flying prep to\n",
      "to pobj Frisco\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.head.text, token.dep_, token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be interested in tokens marked with ROOT and pobj as those are key to intent recognition in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flown', 'LA']\n",
      "['flying', 'Frisco']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I have flown to LA. Now I am flying to Frisco')\n",
    "for sent in doc.sents:\n",
    "    print([w.text for w in sent if w.dep_ == 'ROOT' or w.dep_ == 'pobj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try This\n",
    "We can combine the examples from preceding sections into a single script to correctly identify speakers intent to fly to San Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LA']\n",
      "['fly', 'Frisco']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I have flown to LA. Now I am flying to Frisco')\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"TEXT\": \"Frisco\"}]], {\"LEMMA\": \"San Francisco\"})\n",
    "for sent in doc.sents:\n",
    "    print([w.lemma_ for w in sent if w.dep_ == 'ROOT' and w.tag_ == 'VBG' or w.dep_ == 'pobj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "We can find named entities in a sentence using the ent_type attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA GPE\n",
      "Frisco ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I have flown to LA. Now I am flying to Frisco')\n",
    "for token in doc:\n",
    "    if token.ent_type != 0:\n",
    "        print(token.text, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPE means the entity is a geopolitical entity"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b083a8ac7a1c63ebc5b7ede2fc0321bba53d5d9a8ace42ce490667eb553f9a2c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
