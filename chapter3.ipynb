{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with container objects and customizing spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token, Span and Doc are the most widely used container objects in Spacy. They represent a token, phrase or sentence and a text, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a Doc object using its constructor explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hi there "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.vocab import Vocab\n",
    "doc = Doc(Vocab(), words=[u'Hi', u'there'])\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating over a Token's Syntactic Children\n",
    "We can obtain the leftward syntatic children of the word \"apple\" in the sample sentence below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a, green]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I want a green apple')\n",
    "print([w for w in doc[4].lefts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use Token.rights to find the rightward child of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[apple]\n"
     ]
    }
   ],
   "source": [
    "print([w for w in doc[1].rights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Doc objects doc.sents seperates a text into individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A, severe, storm, hit, the, beach, .]\n",
      "[It, started, to, rain, .]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'A severe storm hit the beach. It started to rain.')\n",
    "for sent in doc.sents:\n",
    "    print([sent[i] for i in range(len(sent))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still refer to the tokens in a multi sentence text using the global document level indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A, severe, storm, hit, the, beach, ., It, started, to, rain, .]\n"
     ]
    }
   ],
   "source": [
    "print([doc[i] for i in range(len(doc))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to refer to the Token objects in a doc by their sentence level indices is useful if, for example, we need to check whether the first word in the second sentence of the text being processed is a pronoun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second sentence begins with a pronoun.\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(doc.sents):\n",
    "    if i == 1 and sent[0].pos_ == 'PRON':\n",
    "        print('The second sentence begins with a pronoun.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify how many sentences end with a verb by checking the last word in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for sent in doc.sents:\n",
    "    if sent[len(sent) - 2].pos_ == 'VERB':\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we reduced the value of len(sent) by 2 because the indices end at size-1 and the last token in the sentences is a period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The doc.noun_chunks container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Doc objects doc.noun_chunks property allows us to iterate over the noun chunks in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A noun chunk\n",
      "a phrase\n",
      "that\n",
      "a noun\n",
      "its head\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'A noun chunk is a phrase that has a noun as its head.')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we might extract noun chunks by iterating over the nouns in the sentence and finding the syntactic children for each noun to form a chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun\n",
      "A chunk\n",
      "a phrase\n",
      "a noun\n",
      "head\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        chunk = ''\n",
    "        for w in token.children:\n",
    "            if w.pos_ == 'DET' or w.pos_ == 'ADJ':\n",
    "                chunk = chunk + w.text + ' '\n",
    "        chunk = chunk + token.text\n",
    "        print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite the previous example to use token.left and remove the check for the children to be a determiner or adjective. This is because the words used to modify a noun are always the leftward syntactic children of the noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Span Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The span object is a slice from a Doc object. One of it's most interesting methods is span.merge which allows us to merge the span into a single token and retokenizes the document. This is useful when the text contains names consisting of several words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The, Golden, Gate, Bridge, is, an, iconic, landmark, in, San, Francisco, .]\n",
      "The the DET det\n",
      "Golden Gate Bridge Golden Gate Bridge PROPN nsubj\n",
      "is be AUX ROOT\n",
      "an an DET det\n",
      "iconic iconic ADJ amod\n",
      "landmark landmark NOUN attr\n",
      "in in ADP prep\n",
      "San San PROPN compound\n",
      "Francisco Francisco PROPN pobj\n",
      ". . PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'The Golden Gate Bridge is an iconic landmark in San Francisco.')\n",
    "print([doc[i] for i in range(len(doc))])\n",
    "span = doc[1:4]\n",
    "lem_id = doc.vocab.strings[span.text]\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[1:4])\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try this\n",
    "Let's see how we can retokenize and merge the San Francisco token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the DET det\n",
      "Golden Gate Bridge Golden Gate Bridge PROPN nsubj\n",
      "is be AUX ROOT\n",
      "an an DET det\n",
      "iconic iconic ADJ amod\n",
      "landmark landmark NOUN attr\n",
      "in in ADP prep\n",
      "San Francisco San Francisco PROPN pobj\n",
      ". . PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[7:9])\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing the text-processing pipeline\n",
    "We can see what pipeline components are available for your nlp object like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can disable pipeline components. Here we create a processing pipeline without a dependency parser. When we call the nlp instance on the text, the tokens dont receie the dependency labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON \n",
      "want VERB \n",
      "a DET \n",
      "green ADJ \n",
      "apple NOUN \n",
      ". PUNCT \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser'])\n",
    "\n",
    "doc = nlp(u'I want a green apple.')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load does a few things behind the scenes.\n",
    "1. Looks at name of model to be loaded and what Language class it should initialize.\n",
    "2. Iterate over processing pipeline names and create corresponding components to add to the processing pipeline.\n",
    "3. Load the model data from disk and make it available to the Language class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can piece together the model name as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_core_web_sm\n"
     ]
    }
   ],
   "source": [
    "print(nlp.meta['lang'] + '_' + nlp.meta['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the meta attribute contains metadata of the language model. We can find the location of our model using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jose/VSCodeProjects/NLPwithPythonandSpacy/venv/lib/python3.10/site-packages/en_core_web_sm')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy import util\n",
    "util.get_package_path('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know the model version since this gives us the folder name of where the model is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_core_web_sm-3.3.0\n"
     ]
    }
   ],
   "source": [
    "print(nlp.meta['lang'] + '_' + nlp.meta['name'] + '-' + nlp.meta['version'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the list of components used w/ the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.meta['pipeline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy.pipeline.tagger.Tagger object at 0x7f876b44f040> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jose/VSCodeProjects/NLPwithPythonandSpacy/chapter3.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jose/VSCodeProjects/NLPwithPythonandSpacy/chapter3.ipynb#ch0000043vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m pipeline:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jose/VSCodeProjects/NLPwithPythonandSpacy/chapter3.ipynb#ch0000043vscode-remote?line=6'>7</a>\u001b[0m     component \u001b[39m=\u001b[39m nlp\u001b[39m.\u001b[39mcreate_pipe(name)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jose/VSCodeProjects/NLPwithPythonandSpacy/chapter3.ipynb#ch0000043vscode-remote?line=7'>8</a>\u001b[0m     nlp\u001b[39m.\u001b[39;49madd_pipe(component)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jose/VSCodeProjects/NLPwithPythonandSpacy/chapter3.ipynb#ch0000043vscode-remote?line=8'>9</a>\u001b[0m nlp\u001b[39m.\u001b[39mfrom_disk(model_data_path)\n",
      "File \u001b[0;32m~/VSCodeProjects/NLPwithPythonandSpacy/venv/lib/python3.10/site-packages/spacy/language.py:773\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jose/VSCodeProjects/NLPwithPythonandSpacy/venv/lib/python3.10/site-packages/spacy/language.py?line=770'>771</a>\u001b[0m     bad_val \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39m(factory_name)\n\u001b[1;32m    <a href='file:///home/jose/VSCodeProjects/NLPwithPythonandSpacy/venv/lib/python3.10/site-packages/spacy/language.py?line=771'>772</a>\u001b[0m     err \u001b[39m=\u001b[39m Errors\u001b[39m.\u001b[39mE966\u001b[39m.\u001b[39mformat(component\u001b[39m=\u001b[39mbad_val, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m--> <a href='file:///home/jose/VSCodeProjects/NLPwithPythonandSpacy/venv/lib/python3.10/site-packages/spacy/language.py?line=772'>773</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[1;32m    <a href='file:///home/jose/VSCodeProjects/NLPwithPythonandSpacy/venv/lib/python3.10/site-packages/spacy/language.py?line=773'>774</a>\u001b[0m name \u001b[39m=\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m factory_name\n\u001b[1;32m    <a href='file:///home/jose/VSCodeProjects/NLPwithPythonandSpacy/venv/lib/python3.10/site-packages/spacy/language.py?line=774'>775</a>\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponent_names:\n",
      "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy.pipeline.tagger.Tagger object at 0x7f876b44f040> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
     ]
    }
   ],
   "source": [
    "lang = 'en'\n",
    "pipeline = ['tagger', 'parser', 'ner']\n",
    "model_data_path = '/home/jose/VSCodeProjects/NLPwithPythonandSpacy/venv/lib/python3.10/site-packages/en_core_web_sm/en_core_web_sm-3.3.0'\n",
    "lang_cls = spacy.util.get_lang_class(lang)\n",
    "nlp = lang_cls()\n",
    "for name in pipeline:\n",
    "    component = nlp.create_pipe(name)\n",
    "    nlp.add_pipe(component)\n",
    "nlp.from_disk(model_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Festy ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I need a taxi to Festy.')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default this is recognized as an organization. We want it to recognize it as a city district. ORG stands for companies, agencies and other institutions. We can create a new label and give it a training example to show the entity recognizer when to apply the DISTRICT label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = 'DISTRICT'\n",
    "TRAIN_DATA = [\n",
    "    ('We need to deliver it to Festy.', {\n",
    "        'entities': [(25, 30, 'DISTRICT')]\n",
    "    }),\n",
    "    ('I like red oranges', {\n",
    "        'entities': []\n",
    "    })\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is an entity in the sample, provide start and end position and type of entity. In second sample, we have no entity. We can now add the label DISTRICT to the entity recognizer. We must first get the instance of the ner pipeline component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can add a label to it using the .add_label method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.add_label(LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to disable other pipes to make sure only the entity recognizer will be updated during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parser']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.disable_pipes('tagger')\n",
    "nlp.disable_pipes('parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train the entity recognizer using the samples in the TRAIN_DATA we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'English' object has no attribute 'entity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jose/VSCodeProjects/NLPwithPythonandSpacy/chapter3.ipynb Cell 56'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jose/VSCodeProjects/NLPwithPythonandSpacy/chapter3.ipynb#ch0000055vscode-remote?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m nlp\u001b[39m.\u001b[39;49mentity\u001b[39m.\u001b[39mcreate_optimizer()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jose/VSCodeProjects/NLPwithPythonandSpacy/chapter3.ipynb#ch0000055vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jose/VSCodeProjects/NLPwithPythonandSpacy/chapter3.ipynb#ch0000055vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m25\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'English' object has no attribute 'entity'"
     ]
    }
   ],
   "source": [
    "optimizer = nlp.entity.create_optimizer()\n",
    "import random\n",
    "\n",
    "for i in range(25):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        nlp.update([text], [annotations], sgd=optimizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b083a8ac7a1c63ebc5b7ede2fc0321bba53d5d9a8ace42ce490667eb553f9a2c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
